var store = [{
        "title": "Bert paper Review",
        "excerpt":"Bert 01 what is Bert? Bert Architecture State of Art 02 Bert Architecture Pre-training task Input Embedding Encoding Fine-tuning 기존 자연어 처리 신경망 RNN ( 순환 신경망 ) : 각 새로운 입력층마다 hidden layer 를 저장/수정 하여 새로운 정보를 점차 업데이트 해나아가는 신경망 LSTM : RNN의 응용모델로 각 신경망에 hidden...","categories": ["NLP"],
        "tags": ["Bert","Attention","NLP","Transformer","Multi-head Attention"],
        "url": "http://localhost:4000/nlp/Bert-paper-Review/",
        "teaser": null
      }]
