var store = [{
        "title": "Bert paper Review",
        "excerpt":"01 what is Bert? BERT ( Bi-directional Encoder Representation from Transformers) 기존 자연어 처리 신경망 RNN ( 순환 신경망 ) : 각 새로운 입력층마다 hidden layer 를 저장/수정 하여 새로운 정보를 점차 업데이트 해나아가는 신경망 LSTM : RNN의 응용모델로 각 신경망에 hidden state 와 cell state 를 더해주어 오랜시간 이전...","categories": ["NLP"],
        "tags": ["Bert","Attention","NLP","Transformer","Multi-head Attention"],
        "url": "http://localhost:4000/nlp/Bert-paper-Review/",
        "teaser": null
      }]
